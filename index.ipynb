{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e371b6e6-0299-4a22-aacc-ed23e4a6cec0",
   "metadata": {},
   "source": [
    "---\n",
    "title: Analyzing Sentiments from Tweets for Mental Health Support\n",
    "author: Neil Dcruze\n",
    "date: '2023-05-15'\n",
    "image: \"image.png\"\n",
    "description: \"Through machine learning techniques, this project aims to analyze symptoms of depression and anxiety by examining tweets\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ab036-9590-4554-af70-611dc63fb85d",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Your abstract is a one-paragraph summary of the problem you addressed, the approach(es) that you used to address it, and the big-picture results that you obtained.\n",
    "\n",
    "<a href=\"https://github.com/dcruzeneil/Mental-Health-Tweetment\"><b>GitHub Repository: <i>Mental Health Tweetment</i></b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c4a8a-2c98-4045-91c5-4c740f178657",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d8cda-62dc-4419-978c-d958dc4295a3",
   "metadata": {},
   "source": [
    "# Values Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9437d-b965-41ba-8065-d8349cb7ae20",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "#### <font color=\"orange\">Sampling the Data</font>\n",
    "Before, we begin doing anything it is important to understand that the <i>Sentiment140</i> is a huge data set, which makes many computations very time-consuming. Therefore, we are going to sample the data set to select $50000$ data points (rows) randomly. The code which was used to sample the data is:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"datasetOriginal.csv\", encoding=\"latin-1\")\n",
    "df = df.sample(n=50000)\n",
    "df.to_csv(\"dataset.csv\", index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f974c7b-de3f-46a7-a34c-c5b26d9f9cac",
   "metadata": {},
   "source": [
    "#### <font color = \"orange\">Loading the Data and Feature Selection</font>\n",
    "Before we perform any of our evaluations, we must prepare our data. For this, we will be using the <i>loadData</i> method from HelperClass:\n",
    "\n",
    "```python\n",
    "def loadData(self):\n",
    "    columns = ['Sentiment Score', 'Tweet ID', 'Time', 'Query', 'Username', 'Tweet']\n",
    "    df = pd.read_csv(r\"../dataset.csv\", encoding=\"latin-1\", names = columns)\n",
    "    df = df[[\"Sentiment Score\", \"Tweet\"]]\n",
    "    return df\n",
    "```\n",
    "In the above operation, we are assigning adequate feature (column) names to the data. This dataset has a bunch of features, however, we are primarily interested in the: \n",
    "<ol>\n",
    "    <li><i><u>Tweet</u></i> feature: which contains the actual tweet - which will be used for Sentiment Analysis\n",
    "    <li><i><u>Sentiment Score</u></i> feature: which contains the true labels - the actual assigned sentiment for each tweet \n",
    "</ol>\n",
    "\n",
    "Now, we can go ahead and store our data as a <i>pandas</i> data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3127d334-70fd-4982-b2cc-cb0768a7024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from HelperClass import HelperClass\n",
    "\n",
    "hp = HelperClass()\n",
    "df = hp.loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6176a85-adab-4bc0-93b7-efacc126a66d",
   "metadata": {},
   "source": [
    "To get an understanding of what the data looks like, we can go ahead and examine the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95490187-61b8-466f-9a0c-a44e74981931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment Score                                              Tweet\n",
       "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1                0  is upset that he can't update his Facebook by ...\n",
       "2                0  @Kenichan I dived many times for the ball. Man...\n",
       "3                0    my whole body feels itchy and like its on fire \n",
       "4                0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eba485-391e-4e33-843f-286ec3b24b4e",
   "metadata": {},
   "source": [
    "Therefore, we can see that now our data solely consists of the Sentiment Score and the Tweet. To understand the distribution of this data, we can perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa27a1a-aece-41b3-b25b-9a1ac97fe026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment Score\n",
       "0    800000\n",
       "4    800000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Sentiment Score\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e3dc1-a578-473e-89a2-d9a35c353dc2",
   "metadata": {},
   "source": [
    "So, we can clearly see that there are $800000$ tweets which are: Negative ($0$), and there are $800000$ tweets which are: Positive ($4$)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac8c6b-d233-448d-adc1-f5aa7e5ede7a",
   "metadata": {},
   "source": [
    "#### <font color = \"orange\"> Preprocessing Tasks </font>\n",
    "To increase accuracy, and to make our input data better, we perform certain \"preprocessing\" tasks on the raw text data. Peforming the following tasks on the data:\n",
    "<ol>\n",
    "    <li> Casing: converting everything to either uppercase or lowercase\n",
    "    <li> Noise Removal: eliminating unwanted characters such as HTML tags, punctuation marks, special characters, and so on\n",
    "    <li> Tokenization: turning all the tweets into tokens - words separated by spaces\n",
    "    <li> Stopword Removal: ignore common English words (\"and\", \"if\", \"for\"), that are not likely to shape the sentiment of the tweet\n",
    "    <li> Text Normalization (Lemmatization): reducing words to their root form to eliminate variations of the same word\n",
    "        <ul>\n",
    "            <li> Better $\\rightarrow$ Good\n",
    "            <li> Runs/Running/Ran $\\rightarrow$ Run\n",
    "        </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a362d7ce-1b6e-420f-8eb1-1b7fd3bb464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(df.shape[0]):\n",
    "#    df[\"Tweet\"][i] = hp.preprocess(df[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00c893-1145-473d-b712-7e68605aaec3",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">Training Data and Testing Data</font>\n",
    "\n",
    "Before, we proceed with any operation it is important to divide our data into <i>training data</i> and <i>testing data</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92606b0-28bc-4405-a56d-70cb607a3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "dfX_train, dfX_test, dfY_train, dfY_test = train_test_split(df[\"Tweet\"], df[\"Sentiment Score\"], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88948a1a-3fd1-4de3-9f1f-bb450dbcb5f9",
   "metadata": {},
   "source": [
    "The code above splits our entire data: the actual tweets, and the associated sentiment score - into training data and testing data. This data exists in the form of <i>pandas</i> dataframes. The <i>test_size</i> = 0.2, tells the function to save 20% of the data for testing, and the rest for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b5566-2d0c-48fe-b029-a25f72a9bf1b",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">Creating the Term-Document Matrix and the Target Vector</font>\n",
    "Now that we have our training and testing data frames, we want to separate it into the:\n",
    "<ul>\n",
    "    <li> Feature Matrix (X), and\n",
    "    <li> Target Vector (y)\n",
    "</ul>\n",
    "Luckily, for us the sentiment score is already numerically encoded (0 = Negative, and 4 = Positive). Although, we will convert this to 0 and 1 (for, 0 and 4 respectively) to standardize how model predictions work. However, we must note that each individual tweet is like an individual row of the Feature Matrix - X. However, currently it is not represented in that format. To achieve that we have to create a \"Term-Document Matrix\". A term-document matrix is a matrix in which:\n",
    "<ol>\n",
    "    <li>Column: each column represents one term (word) that is present in our complete dataset (corpus) of the tweets\n",
    "    <li>Row: each row contains information about one tweet (document), that is which words are present in that particular tweet\n",
    "</ol>\n",
    "To think of it simply, all the columns collectively represent all the words that have come up in the complete tweet dataset. Each row, contains tells us, out of all the possible words, which ones are present (and with what frequency) in each individual tweet. Therefore, our term-document matrix will be of the order: $\\text{tdm} \\in \\mathbb{R}^{n\\text{x}p}$, where $n$ = number of tweets, and $p$ = unique words across all tweets. \n",
    "\n",
    "<br><u>Important Note</u>: in the above description of the columns, all the words in the \"complete tweet dataset\" refers solely to the training data. This is because when we create the list of vocabulary (all the relevant words present in the tweets), we do not want to look at words in the <i>testing data</i> as that might lead to unwanted biases!\n",
    "\n",
    "To create our term-document matrix, we can use the <i>CountVectorizer</i> from <i>scikit-learn</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a92cc82-7dad-48dd-99b7-46215970e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(min_df = 0.001, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d023f7-6e39-4b8d-8e79-26b77ceb4e08",
   "metadata": {},
   "source": [
    "In the above code, we create an instance of the <i>CountVectorizer</i> class. Here there are some important things to unpack:\n",
    "<ol>\n",
    "    <li>min_df: if a term appears in less than 1% of the dataset - ignore it. As we do not have enough data on this term to understand its role in shaping a tweet's sentiment\n",
    "    <li>ngram_range = (1,2): this tells our model to look at uni-grams (one word at a time), and bi-grams (two words taken together). This is good to understand words that often make sense in pairs\n",
    "</ol>\n",
    "\n",
    "Now, we can go ahead and create our term-document matrix (<i>tdm</i>) and appropriate <i>target vector</i> for the training data and the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf6898d-1d5c-403d-b516-0d82a8b79f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = hp.prepData(dfX_train, dfY_train, cv, True)\n",
    "X_test, y_test = hp.prepData(dfX_test, dfY_test, cv, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d9024-e9bd-4cb9-b1d6-c1dfbdc5598c",
   "metadata": {},
   "source": [
    "The <i>prep_data</i> function, we use for getting the final <i>feature matrix</i> and <i>target vector</i> is defined as:\n",
    "```python\n",
    "def prepData(self, dfX, dfY, vectorizer, train = True):\n",
    "    if train:\n",
    "        vectorizer.fit(dfX)\n",
    "    #creating the term document matrix\n",
    "    counts = vectorizer.transform(dfX) \n",
    "    X = pd.DataFrame(counts.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "    X = X.to_numpy()\n",
    "    y = dfY.to_numpy()\n",
    "    y = 1 * (y==4) \n",
    "    return X, y\n",
    "```\n",
    "There are some things to unpack here:\n",
    "<ol>\n",
    "    <li>The check for training ensures that vocabulary is only created for the training data\n",
    "    <li>Based on this constructed vocabulary, a term-document matrix is constructed (for both training and testing data)\n",
    "    <li>We convert our <i>feature matrix</i> and <i>target vector</i> to numpy objects for ease\n",
    "    <li>We change: $y = \\{0, 4\\}^{n}$ to $y = \\{0, 1\\}^{n}$ for a standard machine learning approach\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443836e3-ce02-4ffd-afa9-f396c31fab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the size of: training and testing samples\n",
    "n_train = 50000\n",
    "n_test = 8000\n",
    "\n",
    "def randomSample(n_train, n_test):\n",
    "    #training data\n",
    "    indices = np.random.choice(X_train.shape[0], n_train, replace=False)\n",
    "    X_trainFinal = X_train[indices, :]\n",
    "    y_trainFinal = y_train[indices]\n",
    "    #testing data\n",
    "    indices = np.random.choice(X_test.shape[0], n_test, replace=False)\n",
    "    #creating new feature matrix and target vector (smaller)\n",
    "    X_testFinal = X_test[indices, :]\n",
    "    y_testFinal = y_test[indices]\n",
    "    return X_trainFinal, y_trainFinal, X_testFinal, y_testFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80911d85-f1da-4191-b523-86d8ebcda633",
   "metadata": {},
   "source": [
    "# What is Support Vector Machine - SVM?\n",
    "\n",
    "Support Vector Machines (SVMs) is a popular machine learning model which is often used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1fdf8-60d5-4b58-af29-5c29ae47df41",
   "metadata": {},
   "source": [
    "# PEGASOS (<font color=\"orange\">P</font>rimal <font color=\"orange\">E</font>stimated sub-<font color=\"orange\">G</font>r<font color=\"orange\">A</font>dient <font color=\"orange\">SO</font>lver for <font color=\"orange\">S</font>VM) : <font color = \"orange\">Primal SVM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2451041-d1d2-41f1-a2a5-3d85730bbf52",
   "metadata": {},
   "source": [
    "In PEGASOS (Primal Form of SVMs), our empirical risk minimization problem is:<br><br>\n",
    "<center> $\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w})\\;$ </center>\n",
    "<br>where, our loss function $L(\\mathbf{w})\\;$ was defined as:<br><br>\n",
    "<center> $L(\\mathbf{w}) = \\frac{\\lambda}{2}||w||^2 + \\frac{1}{n} \\sum_{i = 1}^n \\ell_{\\text{hinge}}(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\;$ </center>\n",
    "<br>Therefore, we can see that there is an added regularization to prevent overfitting and to increase the model's ability to generalize over unseen data. For PEGASOS, the loss function which is generally used is called \"Hinge Loss\", which is defined as:<br><br>\n",
    "$$\n",
    "\\ell_{\\text{hinge}}(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i) = \\text{max}\\{0, 1 - y_i\\langle w, x_i \\rangle\\}\n",
    "$$\n",
    "In this case, our gradient function is:<br><br>\n",
    "$$\n",
    "\\nabla L(\\mathbf{w}) = \\lambda w - \\frac{1}{n} \\sum_{i = 1}^n \\mathbb{1}[y_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle < 1] y_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062afda-2d44-43b4-92dc-7b4ad9e04be9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa90cf1-9e40-4733-88b7-36762add7870",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Now, that we have an understanding of how the PEGASOS (Primal SVM) method works, we can import the code that I wrote and <i>fit</i> it over our training data, then we can look at the accuracy we are able to achieve over the <i>training data</i> and the <i>testing data</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497fe94f-8296-4473-b212-630332f16640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrimalSVM import PrimalSVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#PS = PrimalSVM(0.1)\n",
    "PS = SGDClassifier(loss='hinge', penalty='l2')\n",
    "\n",
    "#sampling using the function above\n",
    "X_trainFin, y_trainFin, X_testFin, y_testFin = randomSample(50000, 8000)\n",
    "\n",
    "#X_train = np.append(X_train, np.ones((X_train.shape[0],1)), axis=1)\n",
    "#X_test = np.append(X_test, np.ones((X_test.shape[0],1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ba8d24-e691-46ad-ac46-198b483693ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 0.7865\n",
      "Accuracy on Testing Data: 0.76466875\n"
     ]
    }
   ],
   "source": [
    "#PS.fit(X_trainFin, y_trainFin, 0.1, 500000, 5000)\n",
    "PS.fit(X_trainFin, y_trainFin)\n",
    "#print(f\"Accuracy on Training Data: {PS.score_history[-1]}\")\n",
    "print(f\"Accuracy on Training Data: {PS.score(X_trainFin, y_trainFin)}\")\n",
    "print(f\"Accuracy on Testing Data: {PS.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95093b4f-b4a6-419f-aa97-89b43aeb0c60",
   "metadata": {},
   "source": [
    "# Results and Experiments\n",
    "In this section, we will be performing certain experiments to see how our model performs. \n",
    "\n",
    "#### <font color=\"orange\">Model Accuracy for Ambiguous Sentences</font>\n",
    "First, let us look at how the trained model does in predicting the sentiments of some sentences which could potentially cause an issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "156adbca-7522-4c38-8ec7-f8d6062b4669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:I am in pain\n",
      "Sentiment:Negative\n",
      "\n",
      "Sentence:I hate that movie\n",
      "Sentiment:Negative\n",
      "\n",
      "Sentence:Today has been a good day\n",
      "Sentiment:Positive\n",
      "\n",
      "Sentence:Yesterday was a pretty bad day\n",
      "Sentiment:Negative\n",
      "\n",
      "Sentence:I am not in pain anymore\n",
      "Sentiment:Negative\n",
      "\n",
      "Sentence:I love animals bad\n",
      "Sentiment:Positive\n",
      "\n",
      "Sentence:I love animals bad bad\n",
      "Sentiment:Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testSentences = [\"I am in pain\", \"I hate that movie\", \"Today has been a good day\", \"Yesterday was a pretty bad day\", \"I am not in pain anymore\", \"I love animals bad\", \"I love animals bad bad\"]\n",
    "\n",
    "for sent in testSentences:\n",
    "    #result = hp.sentencePredict()\n",
    "    tempMatrix = hp.sentencePredict(sent, cv)\n",
    "    result = PS.predict(tempMatrix)\n",
    "    sentiment = \"Negative\"\n",
    "    if(result):\n",
    "        sentiment = \"Positive\"\n",
    "    print(f\"Sentence:{sent}\\nSentiment:{sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ea7eb-dcb3-4a8a-a038-835eaae8b765",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">Evolution of Training Score</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feffa5b-ad25-4126-abb2-d362f0d84063",
   "metadata": {},
   "source": [
    "# Concluding Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddabf4b-f849-4e9a-ab94-c1d9c7a6e871",
   "metadata": {},
   "source": [
    "# Personal Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
